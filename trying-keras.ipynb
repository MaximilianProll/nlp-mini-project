{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, metrics, linear_model\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.utils import np_utils\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196017, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(glove_model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) train data shape\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} train data shape')\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train_data.author.values)\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_data.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in xtrain]\n",
    "xvalid_glove = [sent2vec(x) for x in xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.732 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = linear_model.LogisticRegression(C = 1.0)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word not in glove_model:\n",
    "        continue\n",
    "    embedding_vector = glove_model[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 6s 334us/step - loss: 1.0828 - val_loss: 0.9984\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 5s 272us/step - loss: 0.9726 - val_loss: 0.8494\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 4s 241us/step - loss: 0.8889 - val_loss: 0.7694\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 4s 228us/step - loss: 0.8421 - val_loss: 0.7340\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 4s 240us/step - loss: 0.8239 - val_loss: 0.7262\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 4s 249us/step - loss: 0.7971 - val_loss: 0.6972\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 4s 252us/step - loss: 0.7666 - val_loss: 0.6750\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 4s 253us/step - loss: 0.7541 - val_loss: 0.6588\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 4s 204us/step - loss: 0.7396 - val_loss: 0.6476\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 4s 216us/step - loss: 0.7201 - val_loss: 0.6312\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 4s 228us/step - loss: 0.6965 - val_loss: 0.6194\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 4s 229us/step - loss: 0.6891 - val_loss: 0.6080\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 4s 230us/step - loss: 0.6670 - val_loss: 0.5913\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 4s 235us/step - loss: 0.6546 - val_loss: 0.5792\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 4s 241us/step - loss: 0.6445 - val_loss: 0.5721\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 4s 229us/step - loss: 0.6233 - val_loss: 0.5626\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 4s 217us/step - loss: 0.6053 - val_loss: 0.5562\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 4s 212us/step - loss: 0.5964 - val_loss: 0.5559\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 4s 233us/step - loss: 0.5902 - val_loss: 0.5320\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 4s 253us/step - loss: 0.5794 - val_loss: 0.5409\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 4s 200us/step - loss: 0.5586 - val_loss: 0.5354\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 3s 198us/step - loss: 0.5520 - val_loss: 0.5270\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 4s 214us/step - loss: 0.5414 - val_loss: 0.5206\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 4s 226us/step - loss: 0.5238 - val_loss: 0.5161\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 4s 229us/step - loss: 0.5136 - val_loss: 0.5111\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 4s 226us/step - loss: 0.5046 - val_loss: 0.5080\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 3s 197us/step - loss: 0.5010 - val_loss: 0.5013\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 3s 159us/step - loss: 0.4962 - val_loss: 0.5016\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 3s 172us/step - loss: 0.4800 - val_loss: 0.5012\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 3s 183us/step - loss: 0.4767 - val_loss: 0.4988\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 4s 225us/step - loss: 0.4681 - val_loss: 0.4894\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 4s 206us/step - loss: 0.4671 - val_loss: 0.4969\n",
      "Epoch 33/100\n",
      "17621/17621 [==============================] - 4s 226us/step - loss: 0.4513 - val_loss: 0.4904\n",
      "Epoch 34/100\n",
      "17621/17621 [==============================] - 4s 219us/step - loss: 0.4454 - val_loss: 0.4942\n",
      "Epoch 35/100\n",
      "17621/17621 [==============================] - 4s 224us/step - loss: 0.4388 - val_loss: 0.4940\n",
      "Epoch 36/100\n",
      "17621/17621 [==============================] - 4s 213us/step - loss: 0.4294 - val_loss: 0.4929\n",
      "Epoch 37/100\n",
      "17621/17621 [==============================] - 3s 186us/step - loss: 0.4205 - val_loss: 0.4806\n",
      "Epoch 38/100\n",
      "17621/17621 [==============================] - 3s 188us/step - loss: 0.4278 - val_loss: 0.4776\n",
      "Epoch 39/100\n",
      "17621/17621 [==============================] - 4s 200us/step - loss: 0.4143 - val_loss: 0.4852\n",
      "Epoch 40/100\n",
      "17621/17621 [==============================] - 4s 235us/step - loss: 0.4038 - val_loss: 0.4819\n",
      "Epoch 41/100\n",
      "17621/17621 [==============================] - 3s 192us/step - loss: 0.4024 - val_loss: 0.4773\n",
      "Epoch 42/100\n",
      "17621/17621 [==============================] - 4s 227us/step - loss: 0.3959 - val_loss: 0.4860\n",
      "Epoch 43/100\n",
      "17621/17621 [==============================] - 4s 236us/step - loss: 0.3838 - val_loss: 0.4872\n",
      "Epoch 44/100\n",
      "17621/17621 [==============================] - 4s 235us/step - loss: 0.3793 - val_loss: 0.4901\n",
      "Epoch 45/100\n",
      "17621/17621 [==============================] - 4s 230us/step - loss: 0.3793 - val_loss: 0.4813\n",
      "Epoch 46/100\n",
      "17621/17621 [==============================] - 4s 220us/step - loss: 0.3715 - val_loss: 0.4865\n",
      "Epoch 47/100\n",
      "17621/17621 [==============================] - 4s 211us/step - loss: 0.3610 - val_loss: 0.4910\n",
      "Epoch 48/100\n",
      "17621/17621 [==============================] - 4s 239us/step - loss: 0.3630 - val_loss: 0.4959\n",
      "Epoch 49/100\n",
      "17621/17621 [==============================] - 3s 179us/step - loss: 0.3530 - val_loss: 0.4849\n",
      "Epoch 50/100\n",
      "17621/17621 [==============================] - 4s 227us/step - loss: 0.3644 - val_loss: 0.4900\n",
      "Epoch 51/100\n",
      "17621/17621 [==============================] - 4s 240us/step - loss: 0.3484 - val_loss: 0.4933\n",
      "Epoch 52/100\n",
      "17621/17621 [==============================] - 4s 236us/step - loss: 0.3465 - val_loss: 0.5118\n",
      "Epoch 53/100\n",
      "17621/17621 [==============================] - 4s 239us/step - loss: 0.3444 - val_loss: 0.4955\n",
      "Epoch 54/100\n",
      "17621/17621 [==============================] - 4s 250us/step - loss: 0.3342 - val_loss: 0.5040\n",
      "Epoch 55/100\n",
      "17621/17621 [==============================] - 4s 250us/step - loss: 0.3395 - val_loss: 0.4815\n",
      "Epoch 56/100\n",
      "17621/17621 [==============================] - 5s 259us/step - loss: 0.3275 - val_loss: 0.4931\n",
      "Epoch 57/100\n",
      "17621/17621 [==============================] - 4s 234us/step - loss: 0.3316 - val_loss: 0.4882\n",
      "Epoch 58/100\n",
      "17621/17621 [==============================] - 4s 234us/step - loss: 0.3203 - val_loss: 0.4896\n",
      "Epoch 59/100\n",
      "17621/17621 [==============================] - 4s 204us/step - loss: 0.3164 - val_loss: 0.5154\n",
      "Epoch 60/100\n",
      "17621/17621 [==============================] - 4s 227us/step - loss: 0.3174 - val_loss: 0.5076\n",
      "Epoch 61/100\n",
      "17621/17621 [==============================] - 4s 204us/step - loss: 0.3116 - val_loss: 0.5201\n",
      "Epoch 62/100\n",
      "17621/17621 [==============================] - 4s 227us/step - loss: 0.3176 - val_loss: 0.4985\n",
      "Epoch 63/100\n",
      "17621/17621 [==============================] - 4s 241us/step - loss: 0.3033 - val_loss: 0.5270\n",
      "Epoch 64/100\n",
      "17621/17621 [==============================] - 4s 246us/step - loss: 0.3117 - val_loss: 0.5012\n",
      "Epoch 65/100\n",
      "17621/17621 [==============================] - 4s 233us/step - loss: 0.3091 - val_loss: 0.5205\n",
      "Epoch 66/100\n",
      "17621/17621 [==============================] - 4s 222us/step - loss: 0.3057 - val_loss: 0.5169\n",
      "Epoch 67/100\n",
      "17621/17621 [==============================] - 4s 212us/step - loss: 0.3030 - val_loss: 0.5198\n",
      "Epoch 68/100\n",
      "17621/17621 [==============================] - 3s 182us/step - loss: 0.3033 - val_loss: 0.4996\n",
      "Epoch 69/100\n",
      "17621/17621 [==============================] - 3s 197us/step - loss: 0.2975 - val_loss: 0.5203\n",
      "Epoch 70/100\n",
      "17621/17621 [==============================] - 4s 200us/step - loss: 0.2917 - val_loss: 0.5234\n",
      "Epoch 71/100\n",
      "17621/17621 [==============================] - 4s 213us/step - loss: 0.2846 - val_loss: 0.5239\n",
      "Epoch 72/100\n",
      "17621/17621 [==============================] - 4s 205us/step - loss: 0.2890 - val_loss: 0.5081\n",
      "Epoch 73/100\n",
      "17621/17621 [==============================] - 4s 214us/step - loss: 0.2917 - val_loss: 0.4998\n",
      "Epoch 74/100\n",
      "17621/17621 [==============================] - 4s 202us/step - loss: 0.2774 - val_loss: 0.5281\n",
      "Epoch 75/100\n",
      "17621/17621 [==============================] - 3s 199us/step - loss: 0.2818 - val_loss: 0.5253\n",
      "Epoch 76/100\n",
      "17621/17621 [==============================] - 4s 208us/step - loss: 0.2767 - val_loss: 0.5025\n",
      "Epoch 77/100\n",
      "17621/17621 [==============================] - 4s 203us/step - loss: 0.2796 - val_loss: 0.5144\n",
      "Epoch 78/100\n",
      "17621/17621 [==============================] - 4s 208us/step - loss: 0.2731 - val_loss: 0.5269\n",
      "Epoch 79/100\n",
      "17621/17621 [==============================] - 4s 236us/step - loss: 0.2656 - val_loss: 0.5382\n",
      "Epoch 80/100\n",
      "17621/17621 [==============================] - 4s 212us/step - loss: 0.2657 - val_loss: 0.5454\n",
      "Epoch 81/100\n",
      "17621/17621 [==============================] - 4s 225us/step - loss: 0.2722 - val_loss: 0.5262\n",
      "Epoch 82/100\n",
      "17621/17621 [==============================] - 4s 225us/step - loss: 0.2649 - val_loss: 0.5232\n",
      "Epoch 83/100\n",
      "17621/17621 [==============================] - 3s 195us/step - loss: 0.2596 - val_loss: 0.5246\n",
      "Epoch 84/100\n",
      "17621/17621 [==============================] - 3s 195us/step - loss: 0.2632 - val_loss: 0.5215\n",
      "Epoch 85/100\n",
      "17621/17621 [==============================] - 4s 202us/step - loss: 0.2576 - val_loss: 0.5307\n",
      "Epoch 86/100\n",
      "17621/17621 [==============================] - 4s 223us/step - loss: 0.2650 - val_loss: 0.5270\n",
      "Epoch 87/100\n",
      "17621/17621 [==============================] - 5s 260us/step - loss: 0.2567 - val_loss: 0.5428\n",
      "Epoch 88/100\n",
      "17621/17621 [==============================] - 4s 241us/step - loss: 0.2479 - val_loss: 0.5398\n",
      "Epoch 89/100\n",
      "17621/17621 [==============================] - 4s 247us/step - loss: 0.2525 - val_loss: 0.5371\n",
      "Epoch 90/100\n",
      "17621/17621 [==============================] - 4s 243us/step - loss: 0.2515 - val_loss: 0.5325\n",
      "Epoch 91/100\n",
      "17621/17621 [==============================] - 4s 222us/step - loss: 0.2523 - val_loss: 0.5346\n",
      "Epoch 92/100\n",
      "17621/17621 [==============================] - 3s 194us/step - loss: 0.2458 - val_loss: 0.5486\n",
      "Epoch 93/100\n",
      "17621/17621 [==============================] - 3s 182us/step - loss: 0.2445 - val_loss: 0.5451\n",
      "Epoch 94/100\n",
      "17621/17621 [==============================] - 3s 198us/step - loss: 0.2420 - val_loss: 0.5377\n",
      "Epoch 95/100\n",
      "17621/17621 [==============================] - 4s 199us/step - loss: 0.2467 - val_loss: 0.5428\n",
      "Epoch 96/100\n",
      "17621/17621 [==============================] - 4s 206us/step - loss: 0.2427 - val_loss: 0.5393\n",
      "Epoch 97/100\n",
      "17621/17621 [==============================] - 3s 176us/step - loss: 0.2380 - val_loss: 0.5460\n",
      "Epoch 98/100\n",
      "17621/17621 [==============================] - 3s 184us/step - loss: 0.2379 - val_loss: 0.5536\n",
      "Epoch 99/100\n",
      "17621/17621 [==============================] - 3s 182us/step - loss: 0.2414 - val_loss: 0.5446\n",
      "Epoch 100/100\n",
      "17621/17621 [==============================] - 4s 212us/step - loss: 0.2365 - val_loss: 0.5519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd04424a8d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
