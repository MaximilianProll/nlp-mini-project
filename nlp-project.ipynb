{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CUDA devices:  1\n",
      "Quadro P5000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "# nlp imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import sent2vec\n",
    "\n",
    "# basic classifiers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, linear_model, metrics, naive_bayes, svm\n",
    "\n",
    "# goodsie classifiers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# specify torch to use gpu\n",
    "print('number of CUDA devices: ',torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} train data shape')\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {}\n",
    "for words in train_features:\n",
    "    for word in words.split():\n",
    "        count = frequency_dict.get(word, 0)\n",
    "        frequency_dict[word] = count + 1\n",
    "        \n",
    "sorted_doc = (sorted(frequency_dict.items(), key=operator.itemgetter(1)))[::-1]\n",
    "frequency = []\n",
    "rank = []\n",
    "word_rank = 0\n",
    "word_frequency = 0\n",
    "\n",
    "entry_num = 1\n",
    "for entry in sorted_doc:\n",
    "\n",
    "    if (entry[0] == word):\n",
    "        word_rank = entry_num\n",
    "        word_frequency = entry[1]\n",
    "\n",
    "    rank.append(entry_num)\n",
    "    entry_num += 1\n",
    "    frequency.append(entry[1])\n",
    "    \n",
    "m, c = np.polyfit(np.log(rank), np.log(frequency), 1) # fit log(y) = m*log(x) + c\n",
    "y_fit = np.exp(m * np.log(rank) + c) # calculate the fitted values of y\n",
    "\n",
    "plt.loglog(frequency, label='Dataset')\n",
    "plt.loglog(y_fit, ':', label='Zipf')\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"Word frequencies, Zipf's law\")\n",
    "plt.legend()\n",
    "sns.despine(trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_and_y(raw_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    for sentence_index in range(raw_data.shape[0]):\n",
    "        sentence = raw_data.text[sentence_index]\n",
    "        author = raw_data.author[sentence_index]\n",
    "\n",
    "        # tokenize\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "        # remove stopwords, covert to lowercase\n",
    "        sentence_words = [word.lower() for word in sentence_words if word.lower() not in stopwords]\n",
    "\n",
    "        # remove numbers\n",
    "        words = [re.sub('[0-9]+', '', token) for token in sentence_words]\n",
    "\n",
    "        # lemmatizing\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "\n",
    "        features.append(' '.join(sentence_words))\n",
    "        labels.append(author)\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 19579 training data features and labels\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_X_and_y(train_data)\n",
    "print(f'extracted {len(train_features)} training data features and labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing using tf-idf vectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into train and validation sets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (15663, 22016)\n",
      "validation dataset size: (3916, 22016)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_vectors, train_labels, test_size=0.2, shuffle=True)\n",
    "print(f'training dataset size: {X_train.shape}')\n",
    "print(f'validation dataset size: {X_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Approaches\n",
    "\n",
    "- Logistic Regression (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.6125\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes logloss: 0.5926\n"
     ]
    }
   ],
   "source": [
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'naive bayes logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Neural Network (using BOWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22040 3\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sentence in train_features:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "author_to_ix = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "print(len(word_to_ix), len(author_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(word_to_ix))\n",
    "    for word in sentence.split():\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "def make_target(label, author_to_ix):\n",
    "    return torch.LongTensor([author_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, n_hidden1, n_hidden2):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.hidden1 = nn.Linear(vocab_size, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, num_labels)\n",
    "        \n",
    "    def forward(self, x_val):\n",
    "        x = Variable(x_val, requires_grad=False)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663 training samples, 3915 validation samples\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "HIDDEN1 = 1024\n",
    "HIDDEN2 = 1024\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 1000\n",
    "num_batches = int(len(train_features) * 0.8) // batch_size\n",
    "critize = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]\n",
    "\n",
    "print(f'{len(X_train)} training samples, {len(X_valid)} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep neural network loss:  0.475535\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        X_batch = np.array([make_bow_vector(sentence, word_to_ix) for sentence in X_batch])\n",
    "        y_batch = np.array([make_target(author, author_to_ix) for author in y_batch])\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch), requires_grad=False)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "\n",
    "    X_batch = X_valid\n",
    "    y_batch = y_valid\n",
    "    X_batch = np.array([make_bow_vector(sentence, word_to_ix) for sentence in X_batch])\n",
    "    y_batch = np.array([make_target(author, author_to_ix) for author in y_batch])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    y_batch = Variable(torch.from_numpy(y_batch), requires_grad=False)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    validation_loss = critize(output_fw, y_batch)\n",
    "    \n",
    "    print(f'deep neural network loss:  {validation_loss:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = list(train_data.text.values)\n",
    "train_labels = list(train_data.author.values)\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('torontobooks_bigrams.bin')\n",
    "train_embeddings = model.embed_sentences(X_train)\n",
    "\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('torontobooks_bigrams.bin')\n",
    "valid_embeddings = model.embed_sentences(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence embeddings of dimension 700\n"
     ]
    }
   ],
   "source": [
    "print(f'sentence embeddings of dimension {len(train_embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression (using sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.7818\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(train_embeddings, y_train)\n",
    "predictions = classifier.predict_proba(valid_embeddings)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support vector machines (using sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(probability=True)\n",
    "classifier.fit(train_embeddings, y_train)\n",
    "predictions = classifier.predict_proba(valid_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM logloss: 0.7001\n"
     ]
    }
   ],
   "source": [
    "print(f'SVM logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deep neural network (recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_function, name ='validation'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for index in range(len(X_valid)):\n",
    "        sent = X_valid[index]\n",
    "        label = y_valid[index]\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = Variable(torch.LongTensor([word_to_ix[w] for w in sent.split()]))\n",
    "        label = Variable(torch.LongTensor([author_to_ix[label]]))\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    \n",
    "    avg_loss /= len(X_valid)\n",
    "    print(name + ' avg_loss:%g' % (avg_loss))\n",
    "\n",
    "def train_epoch(model, loss_function, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for index in range(len(X_train)):\n",
    "        sent = X_train[index]\n",
    "        label = y_train[index]\n",
    "        model.hidden = model.init_hidden()\n",
    "        if sent is '':\n",
    "            continue\n",
    "            \n",
    "        sent = Variable(torch.LongTensor([word_to_ix[w] for w in sent.split()]))\n",
    "        label = Variable(torch.LongTensor([author_to_ix[label]]))\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss /= len(train_data)\n",
    "    print('epoch: %d done! \\n train avg_loss:%g'%(epoch, avg_loss))\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    EMBEDDING_DIM = 75\n",
    "    HIDDEN_DIM = 50\n",
    "    EPOCH = 5\n",
    "    best_dev_acc = 0.0\n",
    "    model = LSTMClassifier(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                           vocab_size=VOCAB_SIZE, label_size=NUM_LABELS)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        train_epoch(model, loss_function, optimizer, epoch)\n",
    "        validate(model, loss_function)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 done! \n",
      " train avg_loss:0.682957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation avg_loss:0.688963\n",
      "epoch: 1 done! \n",
      " train avg_loss:0.369792\n",
      "validation avg_loss:0.628053\n",
      "epoch: 2 done! \n",
      " train avg_loss:0.200964\n",
      "validation avg_loss:0.685976\n",
      "epoch: 3 done! \n",
      " train avg_loss:0.103686\n",
      "validation avg_loss:0.827851\n",
      "epoch: 4 done! \n",
      " train avg_loss:0.054936\n",
      "validation avg_loss:1.00635\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_X_and_y(train_data)\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
