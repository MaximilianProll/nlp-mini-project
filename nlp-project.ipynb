{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Prediction \n",
    "(https://www.kaggle.com/c/spooky-author-identification/data) **to be removed...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data and Problem\n",
    "\n",
    "- **Dataset description**:\n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: `Edgar Allan Poe`, `HP Lovecraft` and `Mary Shelley`.\n",
    "\n",
    "- **Problem statement**:\n",
    "\n",
    "Given the training dataset, objective is to design a model that accurately predicts the author of the sentences in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CUDA devices:  1\n",
      "Quadro P5000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# nlp imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "# basic classifiers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, linear_model, metrics, naive_bayes\n",
    "\n",
    "# goodsie classifiers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# specify torch to use gpu\n",
    "print('number of CUDA devices: ',torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Load and check train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) train data shape\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} train data shape')\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Count author frequencies in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e3b7eeb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('author', data = train_data, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it does not make sense to do any further exploratory data analysis without pre-processing, we decided to move to `step-3` and revist `step-2` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "We have used the pre-processing techniques used throughout the course of the study group\n",
    "\n",
    "- tokenize sentences\n",
    "- remove stopwords\n",
    "- remove numbers\n",
    "- convert to lowercase\n",
    "- lemmatizing (instead of `stemming`, as we had enough resources to cope with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "train_features = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 19579 training data features and labels\n"
     ]
    }
   ],
   "source": [
    "for sentence_index in range(train_data.shape[0]):\n",
    "    sentence = train_data.text[sentence_index]\n",
    "    author = train_data.author[sentence_index]\n",
    "    \n",
    "    # tokenize\n",
    "    sentence_words = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # remove stopwords, covert to lowercase\n",
    "    sentence_words = [word.lower() for word in sentence_words if word.lower() not in stopwords]\n",
    "    \n",
    "    # remove numbers\n",
    "    words = [re.sub('[0-9]+', '', token) for token in sentence_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    \n",
    "    train_features.append(' '.join(sentence_words))\n",
    "    train_labels.append(author)\n",
    "    \n",
    "print(f'extracted {len(train_features)} training data features and labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing using tf-idf vectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into train and validation sets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (15663, 22016)\n",
      "validation dataset size: (3916, 22016)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_vectors, train_labels, test_size=0.2, shuffle=True)\n",
    "print(f'training dataset size: {X_train.shape}')\n",
    "print(f'validation dataset size: {X_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Approaches\n",
    "\n",
    "**a) Motivation**: Though our objective was essentially text classification, we discussed to experiment a bit with the representation/format of the dataset before trying out various classification methods\n",
    "\n",
    "**b) Methods**:\n",
    "\n",
    "- Logistic Regression (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.6125\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes logloss: 0.5926\n"
     ]
    }
   ],
   "source": [
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'naive bayes logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Neural Network (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        # softmax, because we are dealing with more than 2 spooky authors\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    all_authors = list()\n",
    "    all_words = list()\n",
    "    \n",
    "    def collect_unique():\n",
    "        for index in range(len(train_features)):\n",
    "            author = train_labels[index]\n",
    "            sentence = train_features[index]\n",
    "            if author not in all_authors:\n",
    "                all_authors.append(author)\n",
    "            \n",
    "            for word in sentence.split(' '):\n",
    "                if word not in all_words:\n",
    "                    all_words.append(word)\n",
    "        \n",
    "    collect_unique()\n",
    "    input_size = len(all_words)\n",
    "    output_size = len(all_authors)\n",
    "    return input_size, output_size, all_words, all_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_2_tensor(sentence):\n",
    "    tensor = torch.zeros(1, input_size)\n",
    "    for word in sentence.split(' '):\n",
    "        if word not in all_words:\n",
    "            continue\n",
    "        tensor[0][all_words.index(word)] = 1\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_painful_tensors(author, sentence):\n",
    "    author_tensor = Variable(torch.LongTensor([all_authors.index(author)]))\n",
    "    sentence_tensor = Variable(sentence_2_tensor(sentence))\n",
    "    author_tensor = author_tensor.cuda()\n",
    "    sentence_tensor = sentence_tensor.cuda()\n",
    "    return author_tensor, sentence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_hole(output, input, deep_nn):\n",
    "    deep_nn.zero_grad()\n",
    "    \n",
    "    output_p = deep_nn(input)\n",
    "    loss = criterion(output_p, output)\n",
    "    loss.backward()\n",
    "\n",
    "    for param in deep_nn.parameters():\n",
    "        param.data.add_(-learning_rate, param.grad.data)\n",
    "\n",
    "    return output, loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(deep_nn, n_epochs = 1):\n",
    "    current_loss = 0\n",
    "    for iter in range(1, n_epochs + 1):\n",
    "        for index in range(len(train_features)):\n",
    "            author = train_labels[index]\n",
    "            sentence = train_features[index]\n",
    "            author_tensor, sentence_tensor = get_painful_tensors(author, sentence)\n",
    "            output, loss = black_hole(author_tensor, sentence_tensor, deep_nn)\n",
    "            current_loss += loss\n",
    "            if index % 500 == 0:\n",
    "                print('current loss = ',round(current_loss, 2))\n",
    "                print(index, ' of ', len(train_features))\n",
    "    \n",
    "    print('current loss = ',round(current_loss, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally! start the computational graph\n",
    "input_size, output_size, all_words, all_authors = prepare_data()\n",
    "learning_rate = 0.005\n",
    "hidden_size = input_size\n",
    "deep_nn = DeepNN(input_size, hidden_size, output_size)\n",
    "deep_nn.cuda(device=device)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss =  1.1\n",
      "0  of  19579\n",
      "current loss =  547.71\n",
      "500  of  19579\n",
      "current loss =  1078.1\n",
      "1000  of  19579\n",
      "current loss =  1612.18\n",
      "1500  of  19579\n",
      "current loss =  2133.54\n",
      "2000  of  19579\n",
      "current loss =  2646.26\n",
      "2500  of  19579\n",
      "current loss =  3156.76\n",
      "3000  of  19579\n",
      "current loss =  3661.56\n",
      "3500  of  19579\n",
      "current loss =  4158.42\n",
      "4000  of  19579\n",
      "current loss =  4639.02\n",
      "4500  of  19579\n",
      "current loss =  5104.89\n",
      "5000  of  19579\n",
      "current loss =  5558.74\n",
      "5500  of  19579\n",
      "current loss =  6001.95\n",
      "6000  of  19579\n",
      "current loss =  6432.49\n",
      "6500  of  19579\n",
      "current loss =  6861.91\n",
      "7000  of  19579\n",
      "current loss =  7280.71\n",
      "7500  of  19579\n",
      "current loss =  7675.82\n",
      "8000  of  19579\n",
      "current loss =  8057.54\n",
      "8500  of  19579\n",
      "current loss =  8440.85\n",
      "9000  of  19579\n",
      "current loss =  8812.61\n",
      "9500  of  19579\n",
      "current loss =  9178.25\n",
      "10000  of  19579\n",
      "current loss =  9532.52\n",
      "10500  of  19579\n",
      "current loss =  9879.91\n",
      "11000  of  19579\n",
      "current loss =  10233.8\n",
      "11500  of  19579\n",
      "current loss =  10581.43\n",
      "12000  of  19579\n",
      "current loss =  10899.46\n",
      "12500  of  19579\n",
      "current loss =  11231.75\n",
      "13000  of  19579\n",
      "current loss =  11570.83\n",
      "13500  of  19579\n",
      "current loss =  11890.56\n",
      "14000  of  19579\n",
      "current loss =  12177.29\n",
      "14500  of  19579\n",
      "current loss =  12486.72\n",
      "15000  of  19579\n",
      "current loss =  12799.38\n",
      "15500  of  19579\n",
      "current loss =  13096.53\n",
      "16000  of  19579\n",
      "current loss =  13392.75\n",
      "16500  of  19579\n",
      "current loss =  13674.76\n",
      "17000  of  19579\n",
      "current loss =  13967.14\n",
      "17500  of  19579\n",
      "current loss =  14249.49\n",
      "18000  of  19579\n",
      "current loss =  14532.6\n",
      "18500  of  19579\n",
      "current loss =  14825.62\n",
      "19000  of  19579\n",
      "current loss =  15106.69\n",
      "19500  of  19579\n",
      "current loss =  15151.7\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 1\n",
    "train(deep_nn, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model for future references\n",
    "torch.save(deep_nn.state_dict(), 'deep_nn_trained_model_' + str(n_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression (using doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experimental Results\n",
    "\n",
    "|                      | logistic regression | naive bayes (multinomial) | deep neural network |\n",
    "| :------------------: | :-----------------: | :-----------------------: | :-----------------: |\n",
    "|       doc2vec        |                     |                           |                     |\n",
    "|        tf-idf        |                     |                           |                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
