{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Prediction \n",
    "(https://www.kaggle.com/c/spooky-author-identification/data) **to be removed...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data and Problem\n",
    "\n",
    "- **Dataset description**:\n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: `Edgar Allan Poe`, `HP Lovecraft` and `Mary Shelley`.\n",
    "\n",
    "- **Problem statement**:\n",
    "\n",
    "Given the training dataset, objective is to design a model that accurately predicts the author of the sentences in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CUDA devices:  1\n",
      "Quadro P5000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# nlp imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import sent2vec\n",
    "\n",
    "# basic classifiers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, linear_model, metrics, naive_bayes, svm\n",
    "\n",
    "# goodsie classifiers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# specify torch to use gpu\n",
    "print('number of CUDA devices: ',torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Load and check train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) train data shape\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} train data shape')\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Count author frequencies in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e3b7eeb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('author', data = train_data, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it does not make sense to do any further exploratory data analysis without pre-processing, we decided to move to `step-3` and revist `step-2` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "We have used the pre-processing techniques used throughout the course of the study group\n",
    "\n",
    "- tokenize sentences\n",
    "- remove stopwords\n",
    "- remove numbers\n",
    "- convert to lowercase\n",
    "- lemmatizing (instead of `stemming`, as we had enough resources to cope with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_and_y(raw_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    for sentence_index in range(raw_data.shape[0]):\n",
    "        sentence = raw_data.text[sentence_index]\n",
    "        author = raw_data.author[sentence_index]\n",
    "\n",
    "        # tokenize\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "        # remove stopwords, covert to lowercase\n",
    "        sentence_words = [word.lower() for word in sentence_words if word.lower() not in stopwords]\n",
    "\n",
    "        # remove numbers\n",
    "        words = [re.sub('[0-9]+', '', token) for token in sentence_words]\n",
    "\n",
    "        # lemmatizing\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "\n",
    "        features.append(' '.join(sentence_words))\n",
    "        labels.append(author)\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 19579 training data features and labels\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_X_and_y(train_data)\n",
    "print(f'extracted {len(train_features)} training data features and labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing using tf-idf vectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into train and validation sets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (15663, 22016)\n",
      "validation dataset size: (3916, 22016)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_vectors, train_labels, test_size=0.2, shuffle=True)\n",
    "print(f'training dataset size: {X_train.shape}')\n",
    "print(f'validation dataset size: {X_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Approaches\n",
    "\n",
    "**a) Motivation**: Though our objective was essentially text classification, we discussed to experiment a bit with the representation/format of the dataset before trying out various classification methods\n",
    "\n",
    "**b) Methods**:\n",
    "\n",
    "- Logistic Regression (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.6125\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes logloss: 0.5926\n"
     ]
    }
   ],
   "source": [
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'naive bayes logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Neural Network (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22040 3\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sentence in train_features:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "author_to_ix = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "print(len(word_to_ix), len(author_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(word_to_ix))\n",
    "    for word in sentence.split():\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec\n",
    "\n",
    "def make_target(label, author_to_ix):\n",
    "    return torch.LongTensor([author_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, n_hidden1, n_hidden2):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.hidden1 = nn.Linear(vocab_size, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, num_labels)\n",
    "        \n",
    "    def forward(self, x_val):\n",
    "        x = Variable(x_val, requires_grad=False)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663 training samples, 3915 validation samples\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "HIDDEN1 = 1024\n",
    "HIDDEN2 = 1024\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE, HIDDEN1, HIDDEN2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 1000\n",
    "num_batches = int(len(train_features) * 0.8) // batch_size\n",
    "critize = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]\n",
    "\n",
    "print(f'{len(X_train)} training samples, {len(X_valid)} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep neural network loss:  0.475535\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in range(num_batches):\n",
    "        start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        X_batch = np.array([make_bow_vector(sentence, word_to_ix) for sentence in X_batch])\n",
    "        y_batch = np.array([make_target(author, author_to_ix) for author in y_batch])\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        y_batch = Variable(torch.from_numpy(y_batch), requires_grad=False)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output_fw = model.forward(X_batch)\n",
    "\n",
    "        loss = critize(output_fw, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "\n",
    "    X_batch = X_valid\n",
    "    y_batch = y_valid\n",
    "    X_batch = np.array([make_bow_vector(sentence, word_to_ix) for sentence in X_batch])\n",
    "    y_batch = np.array([make_target(author, author_to_ix) for author in y_batch])\n",
    "    X_batch = torch.from_numpy(X_batch).float()\n",
    "    y_batch = Variable(torch.from_numpy(y_batch), requires_grad=False)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output_fw = model.forward(X_batch)\n",
    "    validation_loss = critize(output_fw, y_batch)\n",
    "    \n",
    "    print(f'deep neural network loss:  {validation_loss:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = list(train_data.text.values)\n",
    "train_labels = list(train_data.author.values)\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('torontobooks_bigrams.bin')\n",
    "train_embeddings = model.embed_sentences(X_train)\n",
    "\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('torontobooks_bigrams.bin')\n",
    "valid_embeddings = model.embed_sentences(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence embeddings of dimension 700\n"
     ]
    }
   ],
   "source": [
    "print(f'sentence embeddings of dimension {len(train_embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression (using sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.7818\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(train_embeddings, y_train)\n",
    "predictions = classifier.predict_proba(valid_embeddings)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support vector machines (using sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(probability=True)\n",
    "classifier.fit(train_embeddings, y_train)\n",
    "predictions = classifier.predict_proba(valid_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM logloss: 0.7001\n"
     ]
    }
   ],
   "source": [
    "print(f'SVM logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deep neural network (recurrent, using sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_function, name ='validation'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for index in range(len(X_valid)):\n",
    "        sent = X_valid[index]\n",
    "        label = y_valid[index]\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = Variable(torch.LongTensor([word_to_ix[w] for w in sent.split()]))\n",
    "        label = Variable(torch.LongTensor([author_to_ix[label]]))\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    \n",
    "    avg_loss /= len(X_valid)\n",
    "    print(name + ' avg_loss:%g' % (avg_loss))\n",
    "\n",
    "def train_epoch(model, loss_function, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for index in range(len(X_train)):\n",
    "        sent = X_train[index]\n",
    "        label = y_train[index]\n",
    "        model.hidden = model.init_hidden()\n",
    "        if sent is '':\n",
    "            continue\n",
    "            \n",
    "        sent = Variable(torch.LongTensor([word_to_ix[w] for w in sent.split()]))\n",
    "        label = Variable(torch.LongTensor([author_to_ix[label]]))\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss /= len(train_data)\n",
    "    print('epoch: %d done! \\n train avg_loss:%g'%(epoch, avg_loss))\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    EMBEDDING_DIM = 75\n",
    "    HIDDEN_DIM = 50\n",
    "    EPOCH = 5\n",
    "    best_dev_acc = 0.0\n",
    "    model = LSTMClassifier(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                           vocab_size=VOCAB_SIZE, label_size=NUM_LABELS)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        train_epoch(model, loss_function, optimizer, epoch)\n",
    "        validate(model, loss_function)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 done! \n",
      " train avg_loss:0.682957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/74/surikua1/unix/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation avg_loss:0.688963\n",
      "epoch: 1 done! \n",
      " train avg_loss:0.369792\n",
      "validation avg_loss:0.628053\n",
      "epoch: 2 done! \n",
      " train avg_loss:0.200964\n",
      "validation avg_loss:0.685976\n",
      "epoch: 3 done! \n",
      " train avg_loss:0.103686\n",
      "validation avg_loss:0.827851\n",
      "epoch: 4 done! \n",
      " train avg_loss:0.054936\n",
      "validation avg_loss:1.00635\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_X_and_y(train_data)\n",
    "\n",
    "X_train = train_features[:int(len(train_features) * 0.8)]\n",
    "y_train = train_labels[:int(len(train_features) * 0.8)]\n",
    "\n",
    "X_valid = train_features[int(len(train_features) * 0.8) + 1 :]\n",
    "y_valid = train_labels[int(len(train_features) * 0.8) + 1 :]\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experimental Results\n",
    "\n",
    "|                      | logistic regression | naive bayes (multinomial) | deep neural network (feed forward) |\n",
    "| :------------------: | :-----------------: | :-----------------------: | :-----------------: |\n",
    "|       sent2vec       |                     |                           |                     |\n",
    "|        tf-idf / BOW  |                     |                           |                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
