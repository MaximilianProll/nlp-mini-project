{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Prediction \n",
    "(https://www.kaggle.com/c/spooky-author-identification/data) **to be removed...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data and Problem\n",
    "\n",
    "- **Dataset description**:\n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: `Edgar Allan Poe`, `HP Lovecraft` and `Mary Shelley`.\n",
    "\n",
    "- **Problem statement**:\n",
    "\n",
    "Given the training dataset, objective is to design a model that accurately predicts the author of the sentences in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# nlp imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# basic classifiers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, linear_model, metrics, naive_bayes\n",
    "\n",
    "# goodsie classifiers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Load and check train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) train data shape\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} train data shape')\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Count author frequencies in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x112177978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11245f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('author', data = train_data, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it does not make sense to do any further exploratory data analysis without pre-processing, we decided to move to `step-3` and revist `step-2` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "We have used the pre-processing techniques used throughout the course of the study group\n",
    "\n",
    "- tokenize sentences\n",
    "- remove stopwords\n",
    "- remove numbers\n",
    "- convert to lowercase\n",
    "- lemmatizing (instead of `stemming`, as we had enough resources to cope with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "train_features = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 19579 training data features and labels\n"
     ]
    }
   ],
   "source": [
    "for sentence_index in range(train_data.shape[0]):\n",
    "    sentence = train_data.text[sentence_index]\n",
    "    author = train_data.author[sentence_index]\n",
    "    \n",
    "    # tokenize\n",
    "    sentence_words = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # remove stopwords, covert to lowercase\n",
    "    sentence_words = [word.lower() for word in sentence_words if word.lower() not in stopwords]\n",
    "    \n",
    "    # remove numbers\n",
    "    words = [re.sub('[0-9]+', '', token) for token in sentence_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    \n",
    "    train_features.append(' '.join(sentence_words))\n",
    "    train_labels.append(author)\n",
    "    \n",
    "print(f'extracted {len(train_features)} training data features and labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing using tf-idf vectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into train and validation sets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (15663, 22016)\n",
      "validation dataset size: (3916, 22016)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_vectors, train_labels, test_size=0.2, shuffle=True)\n",
    "print(f'training dataset size: {X_train.shape}')\n",
    "print(f'validation dataset size: {X_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Approaches\n",
    "\n",
    "**a) Motivation**: Though our objective was essentially text classification, we discussed to experiment a bit with the representation/format of the dataset before trying out various classification methods\n",
    "\n",
    "**b) Methods**:\n",
    "\n",
    "- Logistic Regression (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression logloss: 0.6298\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(C = 1.0)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'logistic regression logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes logloss: 0.6034\n"
     ]
    }
   ],
   "source": [
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict_proba(X_valid)\n",
    "\n",
    "print(f'naive bayes logloss: {metrics.log_loss(y_valid, predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Neural Network (using tf-idf sentence vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        # softmax, because we are dealing with more than 2 spooky authors\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    all_authors = list()\n",
    "    all_words = list()\n",
    "    \n",
    "    def collect_unique():\n",
    "        for index in range(len(train_features)):\n",
    "            author = train_labels[index]\n",
    "            sentence = train_features[index]\n",
    "            if author not in all_authors:\n",
    "                all_authors.append(author)\n",
    "            \n",
    "            for word in sentence.split(' '):\n",
    "                if word not in all_words:\n",
    "                    all_words.append(word)\n",
    "        \n",
    "    collect_unique()\n",
    "    input_size = len(all_words)\n",
    "    output_size = len(all_authors)\n",
    "    return input_size, output_size, all_words, all_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_painful_tensors(author, sentence):\n",
    "    def sentence_2_tensor(sentence):\n",
    "        tensor = torch.zeros(1, input_size)\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in all_words:\n",
    "                continue\n",
    "            tensor[0][all_words.index(word)] = 1\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    author_tensor = Variable(torch.LongTensor([all_authors.index(author)]))\n",
    "    return author_tensor, Variable(sentence_2_tensor(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_hole(output, input, deep_nn):\n",
    "    deep_nn.zero_grad()\n",
    "    \n",
    "    output_p = deep_nn(input)\n",
    "    loss = criterion(output_p, output)\n",
    "    loss.backward()\n",
    "\n",
    "    for param in deep_nn.parameters():\n",
    "        param.data.add_(-learning_rate, param.grad.data)\n",
    "\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(deep_nn, n_iters = 1):\n",
    "    current_loss = 0\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for index in range(len(train_features)):\n",
    "            author = train_labels[index]\n",
    "            sentence = train_features[index]\n",
    "            author_tensor, sentence_tensor = get_painful_tensors(author, sentence)\n",
    "            output, loss = black_hole(author_tensor, sentence_tensor, deep_nn)\n",
    "            current_loss += loss\n",
    "    \n",
    "    print(f'loss = {round(current_loss)}:0.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally! start the computational graph\n",
    "input_size, output_size, all_words, all_authors = prepare_data()\n",
    "learning_rate = 0.005\n",
    "hidden_size = input_size\n",
    "deep_nn = DeepNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be run on Triton / GPU machines\n",
    "train(deep_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression (using doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experimental Results\n",
    "\n",
    "|                      | logistic regression | naive bayes (multinomial) | deep neural network |\n",
    "| :------------------: | :-----------------: | :-----------------------: | :-----------------: |\n",
    "|       doc2vec        |                     |                           |                     |\n",
    "|        tf-idf        |                     |                           |                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
