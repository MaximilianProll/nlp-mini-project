{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Prediction \n",
    "(https://www.kaggle.com/c/spooky-author-identification/data)\n",
    "\n",
    "Authors: `Héctor Laria Mantecón (662134)`, `Aditya Kaushik (662862)`, `Maximilian Andreas Peter Proll (662529)`\n",
    "\n",
    "Repo: https://github.com/akskuchi/nlp-mini-project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________\n",
    "### 1. Data and Problem\n",
    "\n",
    "- **Dataset description**:\n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: `Edgar Allan Poe`, `HP Lovecraft` and `Mary Shelley`\n",
    "\n",
    "- **Problem statement**:\n",
    "\n",
    "Given the training dataset, objective is to design a model that accurately predicts the author of the sentences for the validation dataset unused during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________\n",
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Load and check train data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Count author frequencies (category frequency / influence) in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e3b7eeb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('author', data = train_data, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it does not make sense to do any further exploratory data analysis without pre-processing, we decided to move to `step-3` and revist `step-2` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Zip's Law (uses `train_features`)**: Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table - [zipf-law](http://www.geoffkirby.co.uk/ZlPFSLAW.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![zipf](zipf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we inferred that for the dataset in context, the occurance of the uncommon words dominates the overall corpus. This could possibly be because of the spooky style literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We used 'pandas' for retrieving the word frequencies and ranking them and eventually fed this data to polyfit function ('matplotlib') that fits a line to the data_\n",
    "\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "We have used the **pre-processing techniques** used throughout the course of the study group\n",
    "\n",
    "- tokenize sentences\n",
    "- remove stopwords\n",
    "- remove numbers\n",
    "- convert to lowercase\n",
    "- lemmatizing (instead of `stemming`, as we had enough resources to cope with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing using tf-idf vectorizer**: We scored the words using tf-idf instead of simple count-vectorizing, to take into consideration importance of tokens that relate the sentence to its author at the core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into train and validation sets**: Because we did not have any explicit test data from the `Kaggle` competition dataset, we decided to split the training data into $90\\%$ training and $10\\%$ validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For the purpose of data preprocessing, we used the following libraries:_\n",
    "- _NLTK_\n",
    "- _SKLEARN_\n",
    "\n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches\n",
    "\n",
    "**a) Motivation**: Though our objective was essentially text classification, we discussed to experiment a bit with the representation/format of the dataset before trying out various classification methods\n",
    "\n",
    "**b) Methods**:\n",
    "\n",
    "* Logistic Regression (with tf-idf vectors): It was a convenient goto method for our problem at hand as it had support for multi-class classification (`one-vs-rest`, `one-vs-one`)\n",
    "\n",
    "* Naive-Bayes (with tf-idf vectors): Similar motivation as logistic regression and the fact that it is computationally less expensive\n",
    "\n",
    "* Deep Neural Network (using bag-of-words): A simple two fully connected non-linear layers (`relu` activations) and an eventual `softmax` for the output layer\n",
    "\n",
    "* Logistic Regression (with sent2vec): Because tf-idf was failing to capture the subtleties between the words of the sentences of different classes, we decided to try out sentence embeddings - [sent2vec](https://github.com/epfml/sent2vec). We used the `torontobooks_bigrams.bin` pre-trained embeddings model\n",
    "\n",
    "* Support Vector Machines (with sent2vec): To try out performance of other classifiers on the same set of embeddings\n",
    "\n",
    "* Deep Recurrent Neural Network (using bag-of-words): The vanilla classifiers were not yielding desired resutls and that formed the motivation for using sequence models that can make use of previous-state to model relations between the sentences\n",
    "\n",
    "* Deep Recurrent Neural Network (using `GloVe` embeddings, batch training): Because the plain RNN above-mentioned was computationally expensive, we wanted to reduce the dimensionality of the sentence representations. We made use of a simple LSTM layer along with two dense (fully connected) layers with respective empirical dropout values. We used pre-trained `GloVe` word-embeddings trained on book-corpus as it falls in the realm of our dataset context\n",
    "\n",
    "* Convolutional Neural Network (using `GloVe` embeddings, batch training): We stumbled upon this intriguing way of classification while reading about RNNs and evaluated its performance on the dataset - [post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), [publication](http://www.aclweb.org/anthology/D14-1181). Though we did not analyze on why and how things worked on this architecture, we thought it would be fair to atleast report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________\n",
    "### 5. Experimental Results\n",
    "\n",
    "All the source code and respective labelling can be found in the following notebooks:\n",
    "\n",
    "- `nlp-project.ipynb`\n",
    "- `project-nlp.ipynb`\n",
    "- `CNN_pytorch.ipynb`\n",
    "\n",
    "Metrics used:\n",
    "\n",
    "- Log-Loss / Categorical cross-entropy was used in the competition\n",
    "\n",
    "\n",
    "|                   approach                   | loss (mean) | accuracy (%)  |\n",
    "| :------------------------------------------: | :---------: | :-----------: |\n",
    "| CNN (using GloVe embeddings, batch training) |    0.80     |     73.60     |\n",
    "| RNN (using GloVe embeddings, batch training) |    0.49     |     **81.56** |\n",
    "|               RNN (using BOWs)               |    0.62     |      N/A      |\n",
    "|             SVM (with sent2vec)              |    0.70     |      N/A      |\n",
    "|     Logistic Regression (with sent2vec)      |    0.78     |      N/A      |\n",
    "|         Feed Forward NN (using BOWs)         |    0.47     |      N/A      |\n",
    "|      Naive-Bayes (with tf-idf vectors)       |    0.59     |      N/A      |\n",
    "|  Logistic Regression (with tf-idf vectors)   |    0.61     |      N/A      ||        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________\n",
    "\n",
    "### 6. Conclusions\n",
    "\n",
    "From the results above it is evident that the RNNs outperformed other methods, both in terms of accuracy and consistency. We tried working towards grid-searching and tuning the parameter space of the network for further improving it. We figured that implementation was less of a husstle by using `keras` instead of `PyTorch` for RNNs.\n",
    "\n",
    "The better performance of this kind of model can be explained as 1) `GloVe` embeddings retain and project overall relations of the whole sentence, while `BOW` or `word2vec` cannot really grasp them. This makes them a good input suite for learning networks, and 2) nature of the RNN, as explained in section 4.\n",
    "\n",
    "We were able to use many concepts pursued as a part of the study group and also experiment and learn a few more. This project has motivated us to get involved with and tackle further challenges related to text-summarization and classification problems. We had access to resources from `panikki` (_QP5000 GPU_) and was ample for the load of our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
