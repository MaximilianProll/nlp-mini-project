{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, metrics, linear_model\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.utils import np_utils\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196017, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(glove_model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) <- train data shape\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print(f'{train_data.shape} <- train data shape')\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train_data.author.values)\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_data.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in xtrain]\n",
    "xvalid_glove = [sent2vec(x) for x in xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic normalization of subtracting mean & scaling to unit variance\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 binarized as -> [ 0.  0.  1.]\n",
      "1 binarized as -> [ 0.  1.  0.]\n",
      "0 binarized as -> [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# convert labels binary matrix format (easy for evaluation when using categorical cross-entropy)\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "print(f'{ytrain[0]} binarized as -> {ytrain_enc[0]}')\n",
    "print(f'{ytrain[3]} binarized as -> {ytrain_enc[3]}')\n",
    "print(f'{ytrain[1]} binarized as -> {ytrain_enc[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her hair was the brightest living gold, and despite the poverty of her clothing, seemed to set a crown of distinction on her head.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:brown\"> sequenced as = </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 560, 8, 1, 5924, 459, 714, 3, 987, 1, 1794, 2, 29, 3695, 98, 4, 326, 5, 2545, 2, 3103, 27, 29, 166]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:violet\"> padded as = </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   29  560    8    1 5924  459  714    3  987    1 1794    2   29 3695\n",
      "   98    4  326    5 2545    2 3103   27   29  166]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "word_to_ix created for **25943** words"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "print(f'{xtrain[0]}')\n",
    "printmd('<span style=\"color:brown\"> sequenced as = </span>')\n",
    "print(f'{xtrain_seq[0]}')\n",
    "printmd('<span style=\"color:violet\"> padded as = </span>')\n",
    "print(f'{xtrain_pad[0]}\\n')\n",
    "\n",
    "printmd('word_to_ix created for **' + str(len(word_index)) + '** words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding matrix shape -> (25944, 300)\n",
      "example: word 'brightest' has index of -> 5924\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#f49542\"> stored as = </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.98229994e-02  -2.04089999e-01   3.91770005e-01  -1.72519997e-01\n",
      "   3.56979996e-01  -4.32429999e-01  -8.56499970e-02   6.00610018e-01\n",
      "  -2.54280001e-01   1.33080006e+00   5.33129990e-01  -2.30560005e-01\n",
      "  -1.89569995e-01  -3.94380018e-02   5.91380009e-03  -2.78990000e-01\n",
      "   1.84570000e-01   3.42770010e-01   2.64090002e-01  -9.01220024e-01\n",
      "   1.54640004e-01  -3.89690012e-01  -2.58080006e-01   1.90899998e-01\n",
      "   9.39050019e-02   3.23870003e-01  -5.19240022e-01   3.02240014e-01\n",
      "  -2.69179996e-02   3.67590010e-01   8.36490020e-02   2.77469993e-01\n",
      "  -2.53490001e-01   2.40429994e-02  -4.54030007e-01   2.16340005e-01\n",
      "  -1.26560003e-01  -5.38709983e-02   4.70470011e-01   1.88040003e-01\n",
      "  -1.96160004e-01   1.89950004e-01  -9.57890004e-02   4.27300006e-01\n",
      "  -4.40200008e-02  -5.31979978e-01   6.56029999e-01   1.65560007e-01\n",
      "   6.88600019e-02  -1.28020000e+00   1.78409994e-01   5.35510004e-01\n",
      "  -4.75500003e-02  -9.73879993e-02   1.29590005e-01  -4.27089989e-01\n",
      "   1.23779997e-01   4.21169996e-01   1.43749997e-01  -1.70450002e-01\n",
      "   1.55599996e-01  -1.17309999e+00  -7.13829994e-02  -1.87209994e-01\n",
      "   5.02200007e-01   1.87619999e-01   2.33219996e-01  -6.15690015e-02\n",
      "   2.97529995e-01   1.04889996e-01   3.01980004e-02  -3.96490008e-01\n",
      "  -5.27710021e-01   3.07130009e-01  -8.07780027e-01   4.68160003e-01\n",
      "   8.52340013e-02  -1.04369998e-01   3.99769992e-01   4.47970003e-01\n",
      "  -6.95640028e-01   3.32109988e-01  -3.59100014e-01   1.01240003e+00\n",
      "   4.61809993e-01  -4.07660007e-01  -7.94099987e-01   2.60030001e-01\n",
      "   4.83920008e-01  -1.82640001e-01   2.47120000e-02   2.39410009e-02\n",
      "   3.27380002e-01   5.91390014e-01  -2.96510011e-01  -4.10129994e-01\n",
      "  -2.88260013e-01   3.33840013e-01   6.02039993e-01   5.42990029e-01\n",
      "   5.02650023e-01   1.07709996e-01   5.03669977e-01  -2.10199997e-01\n",
      "   1.76060006e-01  -1.37629998e+00   8.00440013e-02  -6.04049981e-01\n",
      "  -4.83630002e-02   7.18030035e-02   4.26400006e-02   5.44550009e-02\n",
      "  -1.38549998e-01   1.37180001e-01   1.34890005e-01   9.61920023e-01\n",
      "  -4.18089986e-01   4.68760014e-01  -7.62019992e-01  -3.57280001e-02\n",
      "   3.59819998e-04  -1.38370007e-01   2.42630005e-01   3.53789985e-01\n",
      "   5.18920012e-02   1.88940004e-01   3.06089997e-01  -2.41480004e-02\n",
      "  -1.70719996e-01  -3.23610008e-01  -3.29899997e-01   1.68400005e-01\n",
      "   1.64609998e-01   7.99309984e-02   1.12279996e-01  -4.35909986e-01\n",
      "   6.52249977e-02   2.32940003e-01  -3.48650008e-01  -1.53320000e-01\n",
      "  -1.11870003e+00   2.64690012e-01   3.50200012e-02   4.46889997e-01\n",
      "  -2.57609993e-01  -6.04819991e-02   6.07800007e-01   6.07699990e-01\n",
      "   5.65800011e-01  -2.31920004e-01  -1.46259993e-01  -2.98420012e-01\n",
      "   2.04699993e-01   1.70279995e-01   2.90490001e-01   1.04620002e-01\n",
      "  -1.90339997e-01  -2.71219999e-01  -2.95199990e-01  -5.06500006e-01\n",
      "  -1.23870000e-01  -2.34990001e-01   8.64590034e-02  -3.76659989e-01\n",
      "   8.69729996e-01  -1.48169994e-01  -1.29640000e-02  -6.69239998e-01\n",
      "  -9.79529992e-02   4.13919985e-01   1.72729999e-01  -1.11330003e-01\n",
      "  -5.90469986e-02  -6.78690016e-01  -6.71669990e-02   1.75520003e-01\n",
      "  -2.14200005e-01  -7.57669985e-01  -2.54049987e-01  -1.78969994e-01\n",
      "   3.18379998e-01   6.31120026e-01   3.34390014e-01   3.52870002e-02\n",
      "   1.62660003e-01  -1.58899993e-01  -5.32469988e-01  -3.01499993e-01\n",
      "   3.83859992e-01   2.29939997e-01   1.89119995e-01  -1.56939998e-01\n",
      "   9.69730020e-02  -2.97679991e-01  -2.66369991e-02  -1.59309998e-01\n",
      "   2.83809990e-01   2.51789987e-01   4.63490009e-01  -5.66519976e-01\n",
      "  -4.97700006e-01  -3.76500010e-01   9.21190023e-01   2.68689990e-01\n",
      "  -2.26930007e-01  -2.08079994e-01   1.38339996e-01  -6.34509981e-01\n",
      "  -1.01879999e-01  -2.58540004e-01  -1.60820007e-01   2.02350006e-01\n",
      "  -8.33890021e-01  -3.82360011e-01  -1.70249999e-01   4.03439999e-01\n",
      "  -9.39420015e-02  -7.05190003e-01   4.10510004e-01  -1.20229995e+00\n",
      "   1.37219995e-01  -6.94000006e-01   5.96259981e-02   2.51569986e-01\n",
      "  -1.78590007e-02   2.87409991e-01   2.72249997e-01  -6.27040029e-01\n",
      "   1.40729994e-01  -3.11710000e-01   2.72780001e-01  -8.30470026e-02\n",
      "  -3.51819992e-02   5.47940016e-01  -6.92569971e-01  -1.83449998e-01\n",
      "  -1.88379996e-02  -1.29089996e-01  -2.69560009e-01  -2.48510003e-01\n",
      "  -2.66900003e-01   7.70300031e-02   1.00259997e-01   1.54330000e-01\n",
      "  -4.80140001e-01   7.39449978e-01  -3.72260004e-01   1.59920007e-02\n",
      "  -2.56170005e-01   6.75960004e-01   6.01180010e-02   2.15599999e-01\n",
      "   4.05720016e-03  -2.11769998e-01   8.11109990e-02  -1.97630003e-01\n",
      "  -2.11019993e-01   8.98270011e-01  -2.16150001e-01   2.95190006e-01\n",
      "  -3.32800001e-01   9.99320000e-02  -1.78340003e-01   2.26349998e-02\n",
      "  -2.52460003e-01  -5.98230004e-01   1.02870002e-01  -5.85130006e-02\n",
      "  -4.44620013e-01  -2.08189994e-01  -4.88180012e-01   7.52740026e-01\n",
      "   1.59240007e-01  -4.14889991e-01  -1.61760002e-01  -5.38320005e-01\n",
      "   1.88470006e-01   2.87900001e-01  -2.64050007e-01   1.73319995e-01\n",
      "   1.76149994e-01  -3.27859998e-01  -4.10959989e-01   2.14220002e-01\n",
      "   2.77509987e-01  -6.47520006e-01   2.49779999e-01   1.01599999e-01\n",
      "   4.97740000e-01   5.12449980e-01  -1.03989995e+00  -3.39819998e-01\n",
      "  -1.33560002e-01  -1.03299998e-01  -3.07260007e-01   1.70159996e-01\n",
      "   1.06650002e-01   8.51600021e-02  -1.44319996e-01   5.05370021e-01]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word not in glove_model:\n",
    "        continue\n",
    "    embedding_vector = glove_model[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(f'embedding matrix shape -> {embedding_matrix.shape}')\n",
    "print(f'example: word \\'brightest\\' has index of -> {word_index[\"brightest\"]}')\n",
    "printmd('<span style=\"color:#f49542\"> stored as = </span>')\n",
    "print(f'{embedding_matrix[word_index[\"brightest\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/40\n",
      "17621/17621 [==============================] - 18s 1ms/step - loss: 1.0144 - acc: 0.4829 - val_loss: 0.8381 - val_acc: 0.6476\n",
      "Epoch 2/40\n",
      "17621/17621 [==============================] - 17s 939us/step - loss: 0.8737 - acc: 0.6093 - val_loss: 0.7385 - val_acc: 0.6997\n",
      "Epoch 3/40\n",
      "17621/17621 [==============================] - 17s 972us/step - loss: 0.8227 - acc: 0.6429 - val_loss: 0.6931 - val_acc: 0.7145\n",
      "Epoch 4/40\n",
      "17621/17621 [==============================] - 17s 971us/step - loss: 0.7863 - acc: 0.6623 - val_loss: 0.6732 - val_acc: 0.7247\n",
      "Epoch 5/40\n",
      "17621/17621 [==============================] - 17s 980us/step - loss: 0.7514 - acc: 0.6804 - val_loss: 0.6640 - val_acc: 0.7227\n",
      "Epoch 6/40\n",
      "17621/17621 [==============================] - 17s 953us/step - loss: 0.7224 - acc: 0.6959 - val_loss: 0.6218 - val_acc: 0.7477\n",
      "Epoch 7/40\n",
      "17621/17621 [==============================] - 17s 986us/step - loss: 0.6946 - acc: 0.7064 - val_loss: 0.6122 - val_acc: 0.7554\n",
      "Epoch 8/40\n",
      "17621/17621 [==============================] - 17s 956us/step - loss: 0.6712 - acc: 0.7201 - val_loss: 0.5907 - val_acc: 0.7589\n",
      "Epoch 9/40\n",
      "17621/17621 [==============================] - 17s 965us/step - loss: 0.6431 - acc: 0.7350 - val_loss: 0.5693 - val_acc: 0.7676\n",
      "Epoch 10/40\n",
      "17621/17621 [==============================] - 17s 975us/step - loss: 0.6230 - acc: 0.7424 - val_loss: 0.5640 - val_acc: 0.7727\n",
      "Epoch 11/40\n",
      "17621/17621 [==============================] - 17s 971us/step - loss: 0.5961 - acc: 0.7525 - val_loss: 0.5429 - val_acc: 0.7865\n",
      "Epoch 12/40\n",
      "17621/17621 [==============================] - 17s 982us/step - loss: 0.5886 - acc: 0.7578 - val_loss: 0.5490 - val_acc: 0.7829\n",
      "Epoch 13/40\n",
      "17621/17621 [==============================] - 18s 1ms/step - loss: 0.5663 - acc: 0.7667 - val_loss: 0.5365 - val_acc: 0.7865\n",
      "Epoch 14/40\n",
      "17621/17621 [==============================] - 17s 982us/step - loss: 0.5496 - acc: 0.7777 - val_loss: 0.5389 - val_acc: 0.7835\n",
      "Epoch 15/40\n",
      "17621/17621 [==============================] - 18s 1ms/step - loss: 0.5369 - acc: 0.7779 - val_loss: 0.5277 - val_acc: 0.7926\n",
      "Epoch 16/40\n",
      "17621/17621 [==============================] - 17s 981us/step - loss: 0.5271 - acc: 0.7868 - val_loss: 0.5123 - val_acc: 0.7962\n",
      "Epoch 17/40\n",
      "17621/17621 [==============================] - 18s 996us/step - loss: 0.5130 - acc: 0.7923 - val_loss: 0.5130 - val_acc: 0.7978\n",
      "Epoch 18/40\n",
      "17621/17621 [==============================] - 17s 953us/step - loss: 0.4938 - acc: 0.8031 - val_loss: 0.5041 - val_acc: 0.7962\n",
      "Epoch 19/40\n",
      "17621/17621 [==============================] - 17s 941us/step - loss: 0.4812 - acc: 0.8064 - val_loss: 0.5070 - val_acc: 0.8008\n",
      "Epoch 20/40\n",
      "17621/17621 [==============================] - 17s 975us/step - loss: 0.4757 - acc: 0.8105 - val_loss: 0.5414 - val_acc: 0.7799\n",
      "Epoch 21/40\n",
      "17621/17621 [==============================] - 17s 942us/step - loss: 0.4679 - acc: 0.8123 - val_loss: 0.5015 - val_acc: 0.8023\n",
      "Epoch 22/40\n",
      "17621/17621 [==============================] - 17s 968us/step - loss: 0.4447 - acc: 0.8249 - val_loss: 0.4972 - val_acc: 0.8049\n",
      "Epoch 23/40\n",
      "17621/17621 [==============================] - 17s 987us/step - loss: 0.4403 - acc: 0.8255 - val_loss: 0.5013 - val_acc: 0.8064\n",
      "Epoch 24/40\n",
      "17621/17621 [==============================] - 17s 980us/step - loss: 0.4394 - acc: 0.8288 - val_loss: 0.4929 - val_acc: 0.8069\n",
      "Epoch 25/40\n",
      "17621/17621 [==============================] - 17s 975us/step - loss: 0.4213 - acc: 0.8318 - val_loss: 0.4970 - val_acc: 0.8085\n",
      "Epoch 26/40\n",
      "17621/17621 [==============================] - 17s 961us/step - loss: 0.4133 - acc: 0.8351 - val_loss: 0.4943 - val_acc: 0.8008\n",
      "Epoch 27/40\n",
      "17621/17621 [==============================] - 17s 971us/step - loss: 0.4118 - acc: 0.8363 - val_loss: 0.4898 - val_acc: 0.8115\n",
      "Epoch 28/40\n",
      "17621/17621 [==============================] - 18s 997us/step - loss: 0.4057 - acc: 0.8396 - val_loss: 0.4942 - val_acc: 0.8121\n",
      "Epoch 29/40\n",
      "17621/17621 [==============================] - 17s 969us/step - loss: 0.3967 - acc: 0.8417 - val_loss: 0.4902 - val_acc: 0.8100\n",
      "Epoch 30/40\n",
      "17621/17621 [==============================] - 17s 992us/step - loss: 0.3913 - acc: 0.8480 - val_loss: 0.4819 - val_acc: 0.8013\n",
      "Epoch 31/40\n",
      "17621/17621 [==============================] - 17s 979us/step - loss: 0.3824 - acc: 0.8507 - val_loss: 0.4980 - val_acc: 0.8039\n",
      "Epoch 32/40\n",
      "17621/17621 [==============================] - 17s 988us/step - loss: 0.3856 - acc: 0.8497 - val_loss: 0.5008 - val_acc: 0.8075\n",
      "Epoch 33/40\n",
      "17621/17621 [==============================] - 17s 956us/step - loss: 0.3725 - acc: 0.8509 - val_loss: 0.4816 - val_acc: 0.8115\n",
      "Epoch 34/40\n",
      "17621/17621 [==============================] - 17s 987us/step - loss: 0.3655 - acc: 0.8576 - val_loss: 0.5116 - val_acc: 0.8049\n",
      "Epoch 35/40\n",
      "17621/17621 [==============================] - 17s 952us/step - loss: 0.3581 - acc: 0.8616 - val_loss: 0.4937 - val_acc: 0.8090\n",
      "Epoch 36/40\n",
      "17621/17621 [==============================] - 17s 993us/step - loss: 0.3569 - acc: 0.8607 - val_loss: 0.4945 - val_acc: 0.8044\n",
      "Epoch 37/40\n",
      "17621/17621 [==============================] - 17s 983us/step - loss: 0.3597 - acc: 0.8594 - val_loss: 0.5002 - val_acc: 0.8080\n",
      "Epoch 38/40\n",
      "17621/17621 [==============================] - 17s 993us/step - loss: 0.3384 - acc: 0.8694 - val_loss: 0.5070 - val_acc: 0.7983\n",
      "Epoch 39/40\n",
      "17621/17621 [==============================] - 17s 948us/step - loss: 0.3418 - acc: 0.8660 - val_loss: 0.5031 - val_acc: 0.8141\n",
      "Epoch 40/40\n",
      "17621/17621 [==============================] - 17s 992us/step - loss: 0.3344 - acc: 0.8702 - val_loss: 0.4954 - val_acc: 0.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5637c46fd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=128, epochs=40, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/10\n",
      "17621/17621 [==============================] - 7s 405us/step - loss: 0.9088 - acc: 0.5727 - val_loss: 0.7933 - val_acc: 0.6491\n",
      "Epoch 2/10\n",
      "17621/17621 [==============================] - 6s 366us/step - loss: 0.6621 - acc: 0.7181 - val_loss: 0.6455 - val_acc: 0.7360\n",
      "Epoch 3/10\n",
      "17621/17621 [==============================] - 6s 367us/step - loss: 0.4862 - acc: 0.8023 - val_loss: 0.6444 - val_acc: 0.7334\n",
      "Epoch 4/10\n",
      "17621/17621 [==============================] - 6s 366us/step - loss: 0.2907 - acc: 0.8887 - val_loss: 0.8021 - val_acc: 0.7314\n",
      "Epoch 5/10\n",
      "17621/17621 [==============================] - 6s 367us/step - loss: 0.1719 - acc: 0.9359 - val_loss: 0.9606 - val_acc: 0.7293\n",
      "Epoch 6/10\n",
      "17621/17621 [==============================] - 6s 368us/step - loss: 0.1191 - acc: 0.9578 - val_loss: 1.2253 - val_acc: 0.7084\n",
      "Epoch 7/10\n",
      "17621/17621 [==============================] - 6s 368us/step - loss: 0.0785 - acc: 0.9716 - val_loss: 1.4917 - val_acc: 0.7150\n",
      "Epoch 8/10\n",
      "17621/17621 [==============================] - 6s 367us/step - loss: 0.0760 - acc: 0.9730 - val_loss: 1.4492 - val_acc: 0.7089\n",
      "Epoch 9/10\n",
      "17621/17621 [==============================] - 6s 368us/step - loss: 0.0545 - acc: 0.9805 - val_loss: 1.3678 - val_acc: 0.7257\n",
      "Epoch 10/10\n",
      "17621/17621 [==============================] - 6s 369us/step - loss: 0.0489 - acc: 0.9836 - val_loss: 1.5084 - val_acc: 0.7222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5635d9bb00>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Input, Model\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='float')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(512, 15, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(512, 15, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(512, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=128, epochs=10, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
